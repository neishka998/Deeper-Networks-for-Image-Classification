# -*- coding: utf-8 -*-
"""CVTrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AWKLTsFhngz1JDK6OpvWFC17jqn1HN4s
"""

# Commented out IPython magic to ensure Python compatibility.
# Imports
import torch
import torchvision
import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions
import numpy as np
from tqdm import tqdm
import torch.utils.data as data
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision import models
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay


# device=torch.device('cuda')
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load the TensorBoard notebook extension
# %load_ext tensorboard
split_val=0.8

# Commented out IPython magic to ensure Python compatibility.
# Using tensorboard to observe the perfomance of each model with different hyperparameters
# %tensorboard --logdir drive/MyDrive/Colab_Notebooks/CW3_02/ResNet

# DOWNLOAD MODEL AND DATA

def model_and_data(dataset_name, normalize_, model_name,data_aug):   

# SELECT MODEL
  if model_name=='vgg11':
    model = models.vgg11(pretrained=False,num_classes=10)

  if model_name=='vgg11_bn':
    model = models.vgg11_bn(pretrained=False,num_classes=10)

  if model_name=='vgg19':
    model = models.vgg19(pretrained=False,num_classes=10)

  if model_name=='vgg19_bn':
    model = models.vgg19_bn(pretrained=False,num_classes=10)

  if model_name=='vgg16_bn':
    model = models.vgg16_bn(pretrained=False,num_classes=10)

  if model_name=='resnet18':
    model = models.resnet18(pretrained=False,num_classes=10)

  if model_name=='resnet34':
    model = models.resnet34(pretrained=False,num_classes=10)

  if model_name=='googlenet':
    model = models.googlenet(pretrained=False,num_classes=10, aux_logits=False)

# BUILD TRANSFORMS:

  #transform = [transforms.Resize(224)]
  #transform.append(transforms.ToTensor())

# AUGMENTATION OPTIONS:

  # if data_aug_horiz:
  #   transform.append(transforms.RandomHorizontalFlip(p=0.5))

  #if data_aug:
    #transform.append(transforms.RandomHorizontalFlip())
    # transform.append(transforms.RandomResizedCrop(224))
    # transforms.RandomRotation(degrees=(90, -90), fill=(0,))

# DATASET SPECIFIC OPTIONS:
 #if model_name in ['vgg11','vgg11_bn','vgg16','vgg16_bn', 'vgg19', 'vgg19_bn']:
  #  transform.append(transforms.Resize(64))

  #if dataset_name == 'MNIST':
    # print("MNIST processing")
    #transform.append(transforms.Lambda(lambda x: x.repeat(3,1,1)))
    #if normalize_:
      #transform.append(transforms.Normalize((0.1307,), (0.3081,)))
  
  #if dataset_name == 'CIFAR10':
    #if normalize_:
      #transform.append(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))  # use ImageNet mean and std values

# MODEL SPECIFIC OPTIONS:

#  if model_name in ['vgg11','vgg11_bn','vgg16','vgg16_bn', 'vgg19', 'vgg19_bn']:
#    transform.append(transforms.Resize((224,224)))

# COMPOSE TRANSFORM:

  train_transforms = transforms.Compose([
                           transforms.Resize(224),                           
                           transforms.RandomRotation(5),
                           transforms.RandomHorizontalFlip(0.5),
                           transforms.RandomCrop(224, padding = 10),
                           transforms.ToTensor(),
                           #transforms.Lambda(lambda x: x.repeat(3, 1, 1)),                           
                           transforms.Normalize(mean = 0.1307, 
                                                std = 0.3081)
                       ])

  test_transforms = transforms.Compose([
                            transforms.Resize(224),
                            transforms.ToTensor(),
                            #transforms.Lambda(lambda x: x.repeat(3, 1, 1)),                           
                            transforms.Normalize(mean = 0.1307, 
                                                  std = 0.3081)
                       ])
  #transform=transforms.Compose(transform)

# GET DATASETS:

  if dataset_name=='MNIST':
    train_dataset=datasets.MNIST(root='./mnist_data/', train=True, transform=train_transforms, download=True)
    #train_dataset=datasets.MNIST('/content/drive/MyDrive/Colab_Notebooks/CW3_02/1_MNIST_data/',train=True, download=False, transform=transform)
    test_dataset=datasets.MNIST(root='./mnist_data/', train=False, transform=test_transforms, download=True)
    #test_dataset=datasets.MNIST('/content/drive/MyDrive/Colab_Notebooks/CW3_02/1_MNIST_data/',train=False, download=False, transform=transform)

  if dataset_name=='CIFAR10':
    train_dataset=datasets.CIFAR10(root='./cifar_data/', train=True, transform=train_transforms, download=True)
    #train_dataset=datasets.cifar.CIFAR10('/content/drive/MyDrive/Colab_Notebooks/CW3_02/3_CIFAR10_data/',train=True, download=False, transform=transform)
    test_dataset=datasets.CIFAR10(root='./cifar_data/', train=False, transform=test_transforms, download=True)
    #test_dataset=datasets.cifar.CIFAR10('/content/drive/MyDrive/Colab_Notebooks/CW3_02/3_CIFAR10_data/',train=False, download=False, transform=transform)

  return model, train_dataset, test_dataset

def create_dataloaders(train_dataset,test_dataset,batch_size):
  train_split=int(len(train_dataset)*split_val)
  val_split=len(train_dataset)-train_split
  train_dataset_TRAIN, train_dataset_VAL= data.random_split(train_dataset,[train_split,val_split])

  train_loader=torch.utils.data.DataLoader(train_dataset_TRAIN,batch_size=batch_size, shuffle=True)
  val_loader=torch.utils.data.DataLoader(train_dataset_VAL,batch_size=batch_size, shuffle=False)
  test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=True)

  return train_loader, val_loader, test_loader

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

def inspect_data_and_model(dataset_name, model_name):

  # INSPECT MODEL
  
  normalize_= True

  model_test, train_dataset_test, test_dataset_test = model_and_data(dataset_name, normalize_, model_name)
  print("Dataset:\t",dataset_name, "\ttrain length:\\tt", len(train_dataset_test))
  print("Dataset:\t",dataset_name, "\ttest length:\t\t", len(test_dataset_test))

  # CREATE TEST DATA LOADERS

  train_loader_test,val_loader_test, test_loader_test = create_dataloaders(train_dataset_test, test_dataset_test,8)
  
  print("Dataset:\t",dataset_name, "\ttrain loader length:\t", len(train_loader_test))
  print("Dataset:\t",dataset_name, "\tval loader length:\t", len(val_loader_test))
  print("Dataset:\t",dataset_name, "\ttest loader length:\t", len(test_loader_test))

  # INSPECT DATA LOADERS

  dataiter = iter(train_loader_test)
  images, labels = dataiter.next()

  if dataset_name=="CIFAR10":
    classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    
  if dataset_name=="MNIST":
    classes = (0,1,2,3,4,5,6,7,8,9)

  # print images
  plt.figure(figsize = (12,1.5))

  imshow(torchvision.utils.make_grid(images))
  print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))

  #print(model_test)

# Training the model
def train(model,dataloader,optimizer,criterion,device):
  model.train()
  size=len(dataloader.dataset)

  loss_sum=0
  correct=0
  print("train batch start")

  for batch , (inputs, labels) in enumerate(dataloader):
    # print("test 1")

    inputs, labels = inputs.to(device), labels.to(device)   
    outputs = model(inputs)                                
    loss= criterion(outputs,labels)

    # BACK PROPAGATE
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # print("correct was:", correct)
    # print("loss_sum was:",loss_sum)
    # print("size is:", size)

    loss_sum += loss.item()
    correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()   

    # print("correct now:", correct)
    # print("loss sum now:", loss_sum)


  acc=100*correct/size 

  # print(acc)

  return loss_sum/size, acc

# Testing the model
def test(model,dataloader,criterion,device): # no optiizer in validation and test

  data_size=len(dataloader.dataset)
  test_loss=0
  test_correct=0
  model.eval()
  
  with torch.no_grad():
    for inputs, labels in dataloader: 
        inputs, labels = inputs.to(device), labels.to(device)   
        outputs = model(inputs)                                                     # pass data to model and get outputs

        test_loss += criterion(outputs,labels).item()                  # capture loss
        # print("running correct",epoch_test_running_correct)

        test_correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()   
  
  test_loss /= data_size
  test_acc=100 * test_correct / data_size 

  return test_loss, test_acc

def evaluate (model, dataloader, writer, device, classes):
  preds = None
  y_test = None
  x_test = None

  with torch.no_grad():
    for inputs,labels in dataloader:
      inputs,labels = inputs.to(device), labels.to(device)
       
      if preds == None:
        preds=model(inputs)
        y_test=labels
        x_test=inputs

      else:
        preds= torch.cat([preds,model(inputs)])
        y_test = torch.cat([y_test,labels]) 
        x_test = torch.cat([x_test,inputs]) 

  pred_label = preds.argmax(1).cpu()
  y_test = y_test.cpu()
  x_test = x_test.cpu()


  cm= confusion_matrix(y_test.cpu(), pred_label.cpu())
  cmd=ConfusionMatrixDisplay(cm,classes)

  fig, ax=plt.subplots(figsize=(10,10))
  cmd.plot(ax=ax)
  plt.savefig(writer.log_dir + '/cm.png')
  writer.add_figure('Confusion matrix', fig)
  
  pos_x, pos_y, pos_pred = get_examples(x_test, y_test, pred_label, positive=True)
  pos_fig = plot_preds(pos_x, pos_y, pos_pred, classes)
  plt.savefig(writer.log_dir + '/pos.png')
  writer.add_figure('Correct predictions', pos_fig)

  neg_x, neg_y, neg_pred = get_examples(x_test, y_test, pred_label, positive=False)
  neg_fig = plot_preds(neg_x, neg_y, neg_pred, classes)
  plt.savefig(writer.log_dir + '/pneg.png')
  writer.add_figure('Incorrect predictions', neg_fig)

def plot_preds(x,y,pred,labels):
  figure, ax=plt.subplots(3,3,figsize=(15,15))
  for i, axis in enumerate(figure.axes):
    axis.imshow(x[i].moveaxis(0,2))
    axis.set_title(f'true: {labels[y[i]]} pred: {labels[pred[i]]}')
    axis.set_xticks(())
    axis.set_yticks(())
  return figure

def get_examples(x,y,pred,positive=True):
  if positive:
    idx= y == pred
  else:
    idx = y != pred

  return x[idx], y[idx], pred[idx]

def run_main (model_name,dataset_name,normalize, epochs,batch_size,lr,opt,crit,mom,lr_factor,data_aug): 
  torch.cuda.empty_cache()
  device = torch.device('cuda')

  # SELECT LOSS FUNCTION
  if crit=="ce":
    criterion=nn.CrossEntropyLoss()

  model, train_dataset, test_dataset = model_and_data(dataset_name, normalize, model_name,data_aug)

  train_loader,val_loader, test_loader = create_dataloaders(train_dataset, test_dataset,batch_size)
  # train_loader=vgg_MNIST_train_loader
  # val_loader=vgg_MNIST_val_loader

  # DEFINE MODEL
  model=model.to(device)     

  # SELECT OPTIMIZER and Momentum value
  momentum=mom
  if opt=="adam":
    optimizer= optim.Adam(model.parameters(),lr)
  if opt=="sgd":
    optimizer=optim.SGD(model.parameters(), lr, momentum=mom)  

  # optimizer= optim.Adam(model.parameters(),lr)
   

  if lr_factor < 1:
    print("lr_scheduler active")
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=lr_factor, patience=0, verbose=True)


  # INITIATE LOGS
  writer_log_time_start=datetime.now()  # time for tensorboard log name
  writer=SummaryWriter(f'drive/MyDrive/Colab_Notebooks/CW3_02/ResNet/CIFAR10_224/{writer_log_time_start.strftime("%y%m%d_%H:%M")}_{model_name}_{dataset_name}_{normalize}_{epochs}_{opt}_{lr}_{batch_size}')
  # writer=SummaryWriter(f'drive/MyDrive/Colab_Notebooks/CW3_02/CW3_02_logdir/{model_name}_{dataset_name}_{normalize}_{epochs}_{lr}_{batch_size}_{writer_log_time_start.strftime("%Y_%b_%d %H:%M")}')


  for epoch in range(epochs):
    train_start_time=datetime.now()
    print("")
    print(model_name,"_",dataset_name,"_",epochs,"_",lr,"_",batch_size,"_",opt,"_",crit)
    print("EPOCH ",epoch," Train Start time:\t", train_start_time.strftime("%Y_%b_%d:  %H:%M:%S:  %f ms"))
    
    train_loss, train_acc= train(model, train_loader, optimizer, criterion, device)
    
    val_start_time=datetime.now()
    print("EPOCH ",epoch," Val Start time:\t", val_start_time.strftime("%Y_%b_%d:  %H:%M:%S:  %f ms"))

    val_loss, val_acc= test(model, val_loader, criterion, device)


    print("LR was:\t\t\t", lr)

    # update lr 
    if lr_factor < 1:
      print("lr scheduler active")
      scheduler.step(val_acc)   #, epoch) 

    print("LR now is:\t\t", lr)

    writer.add_scalar("Loss/01_train",train_loss,epoch)
    writer.add_scalar("Loss/02_val",val_loss,epoch)

    writer.add_scalar("Accuracy/01_train",train_acc,epoch)
    writer.add_scalar("Accuracy/02_val",val_acc,epoch)

    print(f"train acc, train loss:\t\t {train_acc:.5f}\t{train_loss:.5f}\t  epoch: {epoch}")
    print(f"val acc, val loss:\t\t {val_acc:.5f}\t{val_loss:.5f}\t  epoch: {epoch}","##############")

  test_loss, test_acc = test (model, test_loader, criterion, device)

  evaluate(model, test_loader, writer, device, train_dataset.classes)

  writer_log_time_stop=datetime.now()  # time for tensorboard log name
  average_epoch_time=(writer_log_time_stop - writer_log_time_start)/epochs
  print("Average epoch time:\t\t",average_epoch_time.total_seconds())
  writer_epoch_avg_time=average_epoch_time.total_seconds()


  # WRITE TEXT BASED LOGS:

  log_text=f'Test accuracy: {test_acc} Test loss: {test_loss} Val accuracy: {val_acc} Val loss: {val_loss} No. epochs: {epochs}  Start time: {writer_log_time_start.strftime("%Y_%b_%d %H:%M")}  Finish time: {writer_log_time_stop.strftime("%Y_%b_%d %H:%M")}  Avg epoch time: {average_epoch_time.total_seconds()} '
  writer.add_text('',log_text)

  # writer.add_hparams(hparam_dict={'model': model_name,'dataset': dataset_name,'epochs':epochs,'learning_rate': lr,'batch_size': batch_size}, metric_dict={'test_loss': test_loss, 'test_acc': test_acc}, hparam_domain_discrete={'model': model_list_short, 'dataset': dataset_list,'learning_rate': lr_list,'batch_size': batch_size_list})
  
  
  writer.add_hparams(hparam_dict={'model': model_name,'dataset': dataset_name, 'data_aug': data_aug, 'epochs':epochs, 'avg epoch time': writer_epoch_avg_time,'learning_rate': lr,'batch_size': batch_size}, metric_dict={'val_loss': val_loss, 'val_acc': val_acc, 'test_loss': test_loss, 'test_acc': test_acc}, hparam_domain_discrete={'model': model_list_all, 'dataset': dataset_list,'learning_rate': lr_list,'batch_size': batch_size_list})




  print("########## TRAIN VAL TEST END ##########")

  writer.close()

torch.cuda.empty_cache()
    
epoch_list=[30]  #[10,20,30]


model_list_all=['resnet18'] #'vgg19_bn'] #,'resnet18'] #,'googlenet','vgg11','vgg11_bn','vgg16','vgg16_bn']

lr_list=[0.05, 0.01, 0.005] #, 0.001, 0.0005] ###################, 0.005, 0.01, 0.05]                     #     [0.0005,0.001, 0.002, 0.005,0.01,0.05]
batch_size_list= [64] #, 512] #128,256, 512] #, 256, 512] #, 512]            #     [64,128,256,512]

dataset_list=['CIFAR10'] #,'CIFAR10'] #, 'CIFAR10']          #,    ['MNIST','FashionMNIST','CIFAR10']
optimizer_list=["adam","sgd"] #,"adam"] #, "adam"]            #,"adam"]
normalize_list=[True] ##################################
crit_='ce'
mom_=0.9
lrf=0.1
data_aug_list=[False] #,True] #,True]########################

for model_ in model_list_all:
  for dataset_ in dataset_list:
    for normalize in normalize_list:
      for data_aug_ in data_aug_list:
        for epoch in epoch_list:
          for bs in batch_size_list:
            for optimizer_ in optimizer_list:
              for lr in lr_list:
                run_main(model_name=model_,dataset_name=dataset_, normalize=normalize, epochs=epoch,batch_size=bs,lr=lr,opt=optimizer_,crit=crit_,mom=mom_,lr_factor=lrf, data_aug=data_aug_) 

# RUN ONCE
# run_main(model_name="resnet18",dataset_name="MNIST", epochs=10,batch_size=256,lr=0.001,opt="adam",crit="ce",mom=0.9)